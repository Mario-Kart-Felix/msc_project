{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge import Rouge\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "#goldSummaries = glob.glob(r'..data/raw/OpinosisDataset1.0_0/summaries-gold/*/')\n",
    "gold_summaries = os.walk(r'../data/raw/OpinosisDataset1.0_0/summaries-gold')\n",
    "gold_list = next(gold_summaries)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rouge_evaluation(summaryfiles_dir, summaryDir):\n",
    "    summaryFiles = glob.glob(summaryfiles_dir)\n",
    "    rouge_1_scores_list = []\n",
    "    rouge_2_scores_list = []\n",
    "    rouge_l_scores_list = []\n",
    "    for h in summaryFiles:\n",
    "        foldername_search = re.search(r'[^\\\\/:*?\"<>|\\r\\n]+$', h)\n",
    "        foldername = (foldername_search.group()).split('.')[0]\n",
    "        with open(h, 'r') as f:\n",
    "            hypothesis = f.read()\n",
    "            if foldername in gold_list:\n",
    "                files = glob.glob(summaryDir+foldername+'/*')\n",
    "                foldername = ' '.join(foldername.split('_'))\n",
    "                for r in files:\n",
    "                    with open(r, 'r') as f:\n",
    "                        reference = f.read()\n",
    "                        rouge = Rouge()\n",
    "                        scores = rouge.get_scores(hypothesis, reference)[0]\n",
    "                        rouge_1_scores_list.append(pd.DataFrame(scores['rouge-1'], index=[foldername]))\n",
    "                        rouge_2_scores_list.append(pd.DataFrame(scores['rouge-2'], index=[foldername]))\n",
    "                        rouge_l_scores_list.append(pd.DataFrame(scores['rouge-l'], index=[foldername]))\n",
    "                        rouge_1_df = pd.concat(rouge_1_scores_list)\n",
    "                        rouge_2_df = pd.concat(rouge_2_scores_list)\n",
    "                        rouge_l_df = pd.concat(rouge_l_scores_list)\n",
    "    return rouge_1_df, rouge_2_df, rouge_l_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "textrank_rouge_1, textrank_rouge_2, textrank_rouge_l = rouge_evaluation(r'../data/processed/textrank/*', '../data/raw/OpinosisDataset1.0_0/summaries-gold/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexrank_rouge_1, lexrank_rouge_2, lexrank_rouge_l = rouge_evaluation(r'../data/processed/lexrank/*', '../data/raw/OpinosisDataset1.0_0/summaries-gold/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "opinosis_rouge_1, opinosis_rouge_2, opinosis_rouge_l = rouge_evaluation(r'../data/processed/opinosis/*', '../data/raw/OpinosisDataset1.0_0/summaries-gold/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import f_oneway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-way ANOVA\n",
      "=============\n",
      "F value: [ 19.05129924  97.56197074 138.40071076]\n",
      "P value: [8.71384132e-09 3.62617978e-38 1.71626213e-51] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "f, p = f_oneway(textrank_rouge_1, lexrank_rouge_1, opinosis_rouge_l)\n",
    "print ('One-way ANOVA')\n",
    "print ('=============')\n",
    " \n",
    "print ('F value:', f)\n",
    "print ('P value:', p, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-way ANOVA\n",
      "=============\n",
      "F value: [3.30353605 9.80952928 0.43931625]\n",
      "P value: [3.73179369e-02 6.27326588e-05 6.44651755e-01] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "f, p = f_oneway(lexrank_rouge_2, lexrank_rouge_2, opinosis_rouge_2)\n",
    "print ('One-way ANOVA')\n",
    "print ('=============')\n",
    " \n",
    "print ('F value:', f)\n",
    "print ('P value:', p, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-way ANOVA\n",
      "=============\n",
      "F value: [ 2.66195026 23.52298376  0.53429511]\n",
      "P value: [7.05076886e-02 1.28197023e-10 5.86317397e-01] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "f, p = f_oneway(lexrank_rouge_l, lexrank_rouge_l, opinosis_rouge_l)\n",
    "print ('One-way ANOVA')\n",
    "print ('=============')\n",
    " \n",
    "print ('F value:', f)\n",
    "print ('P value:', p, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "textrank_rouge_1['algorithm'] = 'textrank'\n",
    "textrank_rouge_2['algorithm'] = 'textrank'\n",
    "textrank_rouge_l['algorithm'] = 'textrank'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexrank_rouge_1['algorithm'] = 'lexrank'\n",
    "lexrank_rouge_2['algorithm'] = 'lexrank'\n",
    "lexrank_rouge_l['algorithm'] = 'lexrank'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "opinosis_rouge_1['algorithm'] = 'opinosis'\n",
    "opinosis_rouge_2['algorithm'] = 'opinosis'\n",
    "opinosis_rouge_l['algorithm'] = 'opinosis'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_rouge_1_f_measure = pd.concat([textrank_rouge_1[['algorithm', 'f']],\n",
    "                                  lexrank_rouge_1[['algorithm', 'f']],\n",
    "                                  opinosis_rouge_1[['algorithm', 'f']]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_rouge_1_precision = pd.concat([textrank_rouge_1[['algorithm', 'p']],\n",
    "                                  lexrank_rouge_1[['algorithm', 'p']],\n",
    "                                  opinosis_rouge_1[['algorithm', 'p']]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_rouge_1_recall = pd.concat([textrank_rouge_1[['algorithm', 'r']],\n",
    "                                  lexrank_rouge_1[['algorithm', 'r']],\n",
    "                                  opinosis_rouge_1[['algorithm', 'r']]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiple Comparison of Means - Tukey HSD,FWER=0.05\n",
      "=================================================\n",
      " group1   group2  meandiff  lower   upper  reject\n",
      "-------------------------------------------------\n",
      "lexrank  opinosis  0.0117  -0.0125  0.0359 False \n",
      "lexrank  textrank -0.0589  -0.0831 -0.0348  True \n",
      "opinosis textrank -0.0706  -0.0948 -0.0465  True \n",
      "-------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.stats.multicomp import (pairwise_tukeyhsd,\n",
    "                                         MultiComparison)\n",
    "\n",
    "# Set up the data for comparison (creates a specialised object)\n",
    "MultiComp_rouge_1_f = MultiComparison(pd_rouge_1_f_measure['f'], pd_rouge_1_f_measure['algorithm'])\n",
    "\n",
    "# Show all pair-wise comparisons:\n",
    "\n",
    "# Print the comparisons\n",
    "\n",
    "print(MultiComp_rouge_1_f.tukeyhsd().summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiple Comparison of Means - Tukey HSD,FWER=0.05\n",
      "=================================================\n",
      " group1   group2  meandiff  lower   upper  reject\n",
      "-------------------------------------------------\n",
      "lexrank  opinosis  0.0766   0.0421  0.1111  True \n",
      "lexrank  textrank -0.1431  -0.1776 -0.1087  True \n",
      "opinosis textrank -0.2198  -0.2543 -0.1853  True \n",
      "-------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Set up the data for comparison (creates a specialised object)\n",
    "MultiComp_rouge_1_p = MultiComparison(pd_rouge_1_precision['p'], pd_rouge_1_precision['algorithm'])\n",
    "\n",
    "# Show all pair-wise comparisons:\n",
    "\n",
    "# Print the comparisons\n",
    "print(MultiComp_rouge_1_p.tukeyhsd().summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiple Comparison of Means - Tukey HSD,FWER=0.05\n",
      "================================================\n",
      " group1   group2  meandiff  lower  upper  reject\n",
      "------------------------------------------------\n",
      "lexrank  opinosis -0.0299  -0.0608 0.001  False \n",
      "lexrank  textrank  0.1572   0.1263 0.1881  True \n",
      "opinosis textrank  0.1871   0.1562 0.218   True \n",
      "------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Set up the data for comparison (creates a specialised object)\n",
    "MultiComp_rouge_1_r = MultiComparison(pd_rouge_1_recall['r'], pd_rouge_1_recall['algorithm'])\n",
    "\n",
    "# Show all pair-wise comparisons:\n",
    "\n",
    "# Print the comparisons\n",
    "print(MultiComp_rouge_1_r.tukeyhsd().summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_rouge_2_f_measure = pd.concat([textrank_rouge_2[['algorithm', 'f']],\n",
    "                                  lexrank_rouge_2[['algorithm', 'f']],\n",
    "                                  opinosis_rouge_2[['algorithm', 'f']]])\n",
    "pd_rouge_2_precision = pd.concat([textrank_rouge_2[['algorithm', 'p']],\n",
    "                                  lexrank_rouge_2[['algorithm', 'p']],\n",
    "                                  opinosis_rouge_2[['algorithm', 'p']]])\n",
    "pd_rouge_2_recall = pd.concat([textrank_rouge_2[['algorithm', 'r']],\n",
    "                                  lexrank_rouge_2[['algorithm', 'r']],\n",
    "                                  opinosis_rouge_2[['algorithm', 'r']]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiple Comparison of Means - Tukey HSD,FWER=0.05\n",
      "=================================================\n",
      " group1   group2  meandiff  lower   upper  reject\n",
      "-------------------------------------------------\n",
      "lexrank  opinosis  0.0153   0.0006  0.0299  True \n",
      "lexrank  textrank  -0.003  -0.0176  0.0117 False \n",
      "opinosis textrank -0.0182  -0.0329 -0.0036  True \n",
      "-------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Set up the data for comparison (creates a specialised object)\n",
    "MultiComp_rouge_2_f = MultiComparison(pd_rouge_2_f_measure['f'], pd_rouge_2_f_measure['algorithm'])\n",
    "\n",
    "# Show all pair-wise comparisons:\n",
    "\n",
    "# Print the comparisons\n",
    "\n",
    "print(MultiComp_rouge_2_f.tukeyhsd().summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiple Comparison of Means - Tukey HSD,FWER=0.05\n",
      "=================================================\n",
      " group1   group2  meandiff  lower   upper  reject\n",
      "-------------------------------------------------\n",
      "lexrank  opinosis  0.0401   0.0177  0.0625  True \n",
      "lexrank  textrank -0.0218  -0.0442  0.0006 False \n",
      "opinosis textrank -0.0619  -0.0843 -0.0395  True \n",
      "-------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Set up the data for comparison (creates a specialised object)\n",
    "MultiComp_rouge_2_p = MultiComparison(pd_rouge_2_precision['p'], pd_rouge_2_precision['algorithm'])\n",
    "\n",
    "# Show all pair-wise comparisons:\n",
    "\n",
    "# Print the comparisons\n",
    "\n",
    "print(MultiComp_rouge_2_p.tukeyhsd().summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiple Comparison of Means - Tukey HSD,FWER=0.05\n",
      "================================================\n",
      " group1   group2  meandiff  lower  upper  reject\n",
      "------------------------------------------------\n",
      "lexrank  opinosis  0.0053  -0.0142 0.0249 False \n",
      "lexrank  textrank  0.0618   0.0422 0.0814  True \n",
      "opinosis textrank  0.0565   0.0369 0.076   True \n",
      "------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Set up the data for comparison (creates a specialised object)\n",
    "MultiComp_rouge_2_r = MultiComparison(pd_rouge_2_recall['r'], pd_rouge_2_recall['algorithm'])\n",
    "\n",
    "# Show all pair-wise comparisons:\n",
    "\n",
    "# Print the comparisons\n",
    "\n",
    "print(MultiComp_rouge_2_r.tukeyhsd().summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_rouge_l_f_measure = pd.concat([textrank_rouge_l[['algorithm', 'f']],\n",
    "                                  lexrank_rouge_l[['algorithm', 'f']],\n",
    "                                  opinosis_rouge_l[['algorithm', 'f']]])\n",
    "pd_rouge_l_precision = pd.concat([textrank_rouge_l[['algorithm', 'p']],\n",
    "                                  lexrank_rouge_l[['algorithm', 'p']],\n",
    "                                  opinosis_rouge_l[['algorithm', 'p']]])\n",
    "pd_rouge_l_recall = pd.concat([textrank_rouge_l[['algorithm', 'r']],\n",
    "                                  lexrank_rouge_l[['algorithm', 'r']],\n",
    "                                  opinosis_rouge_l[['algorithm', 'r']]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiple Comparison of Means - Tukey HSD,FWER=0.05\n",
      "=================================================\n",
      " group1   group2  meandiff  lower   upper  reject\n",
      "-------------------------------------------------\n",
      "lexrank  opinosis  0.0196  -0.0009  0.0402 False \n",
      "lexrank  textrank -0.0596  -0.0801  -0.039  True \n",
      "opinosis textrank -0.0792  -0.0998 -0.0587  True \n",
      "-------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Set up the data for comparison (creates a specialised object)\n",
    "MultiComp_rouge_l_f = MultiComparison(pd_rouge_l_f_measure['f'], pd_rouge_l_f_measure['algorithm'])\n",
    "\n",
    "# Show all pair-wise comparisons:\n",
    "\n",
    "# Print the comparisons\n",
    "\n",
    "print(MultiComp_rouge_l_f.tukeyhsd().summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiple Comparison of Means - Tukey HSD,FWER=0.05\n",
      "=================================================\n",
      " group1   group2  meandiff  lower   upper  reject\n",
      "-------------------------------------------------\n",
      "lexrank  opinosis  0.0926   0.0601  0.1251  True \n",
      "lexrank  textrank -0.1116  -0.1441 -0.0791  True \n",
      "opinosis textrank -0.2041  -0.2366 -0.1716  True \n",
      "-------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Set up the data for comparison (creates a specialised object)\n",
    "MultiComp_rouge_l_p = MultiComparison(pd_rouge_l_precision['p'], pd_rouge_l_precision['algorithm'])\n",
    "\n",
    "# Show all pair-wise comparisons:\n",
    "\n",
    "# Print the comparisons\n",
    "\n",
    "print(MultiComp_rouge_l_p.tukeyhsd().summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiple Comparison of Means - Tukey HSD,FWER=0.05\n",
      "================================================\n",
      " group1   group2  meandiff  lower  upper  reject\n",
      "------------------------------------------------\n",
      "lexrank  opinosis  -0.01   -0.0386 0.0187 False \n",
      "lexrank  textrank  0.1641   0.1354 0.1928  True \n",
      "opinosis textrank  0.1741   0.1454 0.2028  True \n",
      "------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Set up the data for comparison (creates a specialised object)\n",
    "MultiComp_rouge_l_r = MultiComparison(pd_rouge_l_recall['r'], pd_rouge_l_recall['algorithm'])\n",
    "\n",
    "# Show all pair-wise comparisons:\n",
    "\n",
    "# Print the comparisons\n",
    "\n",
    "print(MultiComp_rouge_l_r.tukeyhsd().summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
