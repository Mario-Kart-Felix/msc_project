{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge import Rouge\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "#goldSummaries = glob.glob(r'..data/raw/OpinosisDataset1.0_0/summaries-gold/*/')\n",
    "gold_summaries = os.walk(r'../data/raw/OpinosisDataset1.0_0/summaries-gold')\n",
    "gold_list = next(gold_summaries)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rouge_evaluation(summaryfiles_dir, summaryDir):\n",
    "    summaryFiles = glob.glob(summaryfiles_dir)\n",
    "    rouge_1_scores_list = []\n",
    "    rouge_2_scores_list = []\n",
    "    rouge_l_scores_list = []\n",
    "    for h in summaryFiles:\n",
    "        foldername_search = re.search(r'[^\\\\/:*?\"<>|\\r\\n]+$', h)\n",
    "        foldername = (foldername_search.group()).split('.')[0]\n",
    "        with open(h, 'r') as f:\n",
    "            hypothesis = f.read()\n",
    "            if foldername in gold_list:\n",
    "                files = glob.glob(summaryDir+foldername+'/*')\n",
    "                foldername = ' '.join(foldername.split('_'))\n",
    "                for r in files:\n",
    "                    with open(r, 'r') as f:\n",
    "                        reference = f.read()\n",
    "                        rouge = Rouge()\n",
    "                        scores = rouge.get_scores(hypothesis, reference)[0]\n",
    "                        rouge_1_scores_list.append(pd.DataFrame(scores['rouge-1'], index=[foldername]))\n",
    "                        rouge_2_scores_list.append(pd.DataFrame(scores['rouge-2'], index=[foldername]))\n",
    "                        rouge_l_scores_list.append(pd.DataFrame(scores['rouge-l'], index=[foldername]))\n",
    "                        rouge_1_df = pd.concat(rouge_1_scores_list)\n",
    "                        rouge_2_df = pd.concat(rouge_2_scores_list)\n",
    "                        rouge_l_df = pd.concat(rouge_l_scores_list)\n",
    "    return rouge_1_df, rouge_2_df, rouge_l_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "def human_rouge():\n",
    "    rouge_1_scores_list = []\n",
    "    rouge_2_scores_list = []\n",
    "    rouge_l_scores_list = []\n",
    "    for folder in gold_list:\n",
    "        allFiles = glob.glob('C:/Users/obiam/MSc_Project/msc_project/data/raw/OpinosisDataset1.0_0/summaries-gold/'+folder + '/*')\n",
    "        for h, r in itertools.combinations(allFiles, 2):\n",
    "            with open(h, 'r') as f:\n",
    "                hypothesis = f.read()\n",
    "            with open(r, 'r') as f:\n",
    "                reference = f.read()\n",
    "                rouge = Rouge()\n",
    "                scores = rouge.get_scores(hypothesis, reference)[0]\n",
    "                rouge_1_scores_list.append(pd.DataFrame(scores['rouge-1'], index=[folder]))\n",
    "                rouge_2_scores_list.append(pd.DataFrame(scores['rouge-2'], index=[folder]))\n",
    "                rouge_l_scores_list.append(pd.DataFrame(scores['rouge-l'], index=[folder]))\n",
    "                rouge_1_df = pd.concat(rouge_1_scores_list)\n",
    "                rouge_2_df = pd.concat(rouge_2_scores_list)\n",
    "                rouge_l_df = pd.concat(rouge_l_scores_list)\n",
    "    return(rouge_1_df, rouge_2_df, rouge_l_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "textrank_rouge_1, textrank_rouge_2, textrank_rouge_l = rouge_evaluation(r'../data/processed/textrank/*', '../data/raw/OpinosisDataset1.0_0/summaries-gold/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexrank_rouge_1, lexrank_rouge_2, lexrank_rouge_l = rouge_evaluation(r'../data/processed/lexrank/*', '../data/raw/OpinosisDataset1.0_0/summaries-gold/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "opinosis_rouge_1, opinosis_rouge_2, opinosis_rouge_l = rouge_evaluation(r'../data/processed/opinosis/*', '../data/raw/OpinosisDataset1.0_0/summaries-gold/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_rouge_1, human_rouge_2, human_rouge_l = human_rouge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import f_oneway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-way ANOVA\n",
      "=============\n",
      "F value: [ 28.43531232 117.56735763 116.64452713]\n",
      "P value: [1.31746767e-12 7.73320398e-45 1.54821528e-44] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "f, p = f_oneway(textrank_rouge_1, lexrank_rouge_1, opinosis_rouge_1)\n",
    "print ('One-way ANOVA')\n",
    "print ('=============')\n",
    " \n",
    "print ('F value:', f)\n",
    "print ('P value:', p, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-way ANOVA\n",
      "=============\n",
      "F value: [2.38162135 8.90234002 0.04781098]\n",
      "P value: [9.31374301e-02 1.51837217e-04 9.53317028e-01] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "f, p = f_oneway(lexrank_rouge_2, lexrank_rouge_2, opinosis_rouge_2)\n",
    "print ('One-way ANOVA')\n",
    "print ('=============')\n",
    " \n",
    "print ('F value:', f)\n",
    "print ('P value:', p, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-way ANOVA\n",
      "=============\n",
      "F value: [ 1.76808725 21.8449424   0.63825938]\n",
      "P value: [1.71408624e-01 6.20770332e-10 5.28513406e-01] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "f, p = f_oneway(lexrank_rouge_l, lexrank_rouge_l, opinosis_rouge_l)\n",
    "print ('One-way ANOVA')\n",
    "print ('=============')\n",
    " \n",
    "print ('F value:', f)\n",
    "print ('P value:', p, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "textrank_rouge_1['algorithm'] = 'textrank'\n",
    "textrank_rouge_2['algorithm'] = 'textrank'\n",
    "textrank_rouge_l['algorithm'] = 'textrank'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexrank_rouge_1['algorithm'] = 'lexrank'\n",
    "lexrank_rouge_2['algorithm'] = 'lexrank'\n",
    "lexrank_rouge_l['algorithm'] = 'lexrank'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "opinosis_rouge_1['algorithm'] = 'opinosis'\n",
    "opinosis_rouge_2['algorithm'] = 'opinosis'\n",
    "opinosis_rouge_l['algorithm'] = 'opinosis'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_rouge_1['algorithm'] = 'opinosis'\n",
    "human_rouge_2['algorithm'] = 'opinosis'\n",
    "human_rouge_l['algorithm'] = 'opinosis'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_rouge_1_f_measure = pd.concat([textrank_rouge_1[['algorithm', 'f']],\n",
    "                                  lexrank_rouge_1[['algorithm', 'f']],\n",
    "                                  opinosis_rouge_1[['algorithm', 'f']],\n",
    "                                 human_rouge_1[['algorithm', 'f']]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_rouge_1_precision = pd.concat([textrank_rouge_1[['algorithm', 'p']],\n",
    "                                  lexrank_rouge_1[['algorithm', 'p']],\n",
    "                                  opinosis_rouge_1[['algorithm', 'p']],\n",
    "                                 human_rouge_1[['algorithm', 'p']]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_rouge_1_recall = pd.concat([textrank_rouge_1[['algorithm', 'r']],\n",
    "                                  lexrank_rouge_1[['algorithm', 'r']],\n",
    "                                  opinosis_rouge_1[['algorithm', 'r']],\n",
    "                              human_rouge_1[['algorithm', 'r']]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiple Comparison of Means - Tukey HSD,FWER=0.05\n",
      "=================================================\n",
      " group1   group2  meandiff  lower   upper  reject\n",
      "-------------------------------------------------\n",
      "lexrank  opinosis  0.0218  -0.0039  0.0474 False \n",
      "lexrank  textrank -0.0625  -0.0938 -0.0313  True \n",
      "opinosis textrank -0.0843  -0.1099 -0.0586  True \n",
      "-------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.stats.multicomp import (pairwise_tukeyhsd,\n",
    "                                         MultiComparison)\n",
    "\n",
    "# Set up the data for comparison (creates a specialised object)\n",
    "MultiComp_rouge_1_f = MultiComparison(pd_rouge_1_f_measure['f'], pd_rouge_1_f_measure['algorithm'])\n",
    "\n",
    "# Show all pair-wise comparisons:\n",
    "\n",
    "# Print the comparisons\n",
    "\n",
    "print(MultiComp_rouge_1_f.tukeyhsd().summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiple Comparison of Means - Tukey HSD,FWER=0.05\n",
      "=================================================\n",
      " group1   group2  meandiff  lower   upper  reject\n",
      "-------------------------------------------------\n",
      "lexrank  opinosis  0.0383   0.0066  0.0701  True \n",
      "lexrank  textrank -0.1474   -0.186 -0.1087  True \n",
      "opinosis textrank -0.1857  -0.2174 -0.1539  True \n",
      "-------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Set up the data for comparison (creates a specialised object)\n",
    "MultiComp_rouge_1_p = MultiComparison(pd_rouge_1_precision['p'], pd_rouge_1_precision['algorithm'])\n",
    "\n",
    "# Show all pair-wise comparisons:\n",
    "\n",
    "# Print the comparisons\n",
    "print(MultiComp_rouge_1_p.tukeyhsd().summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiple Comparison of Means - Tukey HSD,FWER=0.05\n",
      "===============================================\n",
      " group1   group2  meandiff lower  upper  reject\n",
      "-----------------------------------------------\n",
      "lexrank  opinosis  0.0102  -0.02  0.0403 False \n",
      "lexrank  textrank  0.1562  0.1195 0.1929  True \n",
      "opinosis textrank  0.146   0.1159 0.1761  True \n",
      "-----------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Set up the data for comparison (creates a specialised object)\n",
    "MultiComp_rouge_1_r = MultiComparison(pd_rouge_1_recall['r'], pd_rouge_1_recall['algorithm'])\n",
    "\n",
    "# Show all pair-wise comparisons:\n",
    "\n",
    "# Print the comparisons\n",
    "print(MultiComp_rouge_1_r.tukeyhsd().summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_rouge_2_f_measure = pd.concat([textrank_rouge_2[['algorithm', 'f']],\n",
    "                                  lexrank_rouge_2[['algorithm', 'f']],\n",
    "                                  opinosis_rouge_2[['algorithm', 'f']]])\n",
    "pd_rouge_2_precision = pd.concat([textrank_rouge_2[['algorithm', 'p']],\n",
    "                                  lexrank_rouge_2[['algorithm', 'p']],\n",
    "                                  opinosis_rouge_2[['algorithm', 'p']]])\n",
    "pd_rouge_2_recall = pd.concat([textrank_rouge_2[['algorithm', 'r']],\n",
    "                                  lexrank_rouge_2[['algorithm', 'r']],\n",
    "                                  opinosis_rouge_2[['algorithm', 'r']]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiple Comparison of Means - Tukey HSD,FWER=0.05\n",
      "=================================================\n",
      " group1   group2  meandiff  lower   upper  reject\n",
      "-------------------------------------------------\n",
      "lexrank  opinosis  0.0153   0.0006  0.0299  True \n",
      "lexrank  textrank  -0.003  -0.0176  0.0117 False \n",
      "opinosis textrank -0.0182  -0.0329 -0.0036  True \n",
      "-------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Set up the data for comparison (creates a specialised object)\n",
    "MultiComp_rouge_2_f = MultiComparison(pd_rouge_2_f_measure['f'], pd_rouge_2_f_measure['algorithm'])\n",
    "\n",
    "# Show all pair-wise comparisons:\n",
    "\n",
    "# Print the comparisons\n",
    "\n",
    "print(MultiComp_rouge_2_f.tukeyhsd().summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiple Comparison of Means - Tukey HSD,FWER=0.05\n",
      "=================================================\n",
      " group1   group2  meandiff  lower   upper  reject\n",
      "-------------------------------------------------\n",
      "lexrank  opinosis  0.0401   0.0177  0.0625  True \n",
      "lexrank  textrank -0.0218  -0.0442  0.0006 False \n",
      "opinosis textrank -0.0619  -0.0843 -0.0395  True \n",
      "-------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Set up the data for comparison (creates a specialised object)\n",
    "MultiComp_rouge_2_p = MultiComparison(pd_rouge_2_precision['p'], pd_rouge_2_precision['algorithm'])\n",
    "\n",
    "# Show all pair-wise comparisons:\n",
    "\n",
    "# Print the comparisons\n",
    "\n",
    "print(MultiComp_rouge_2_p.tukeyhsd().summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiple Comparison of Means - Tukey HSD,FWER=0.05\n",
      "================================================\n",
      " group1   group2  meandiff  lower  upper  reject\n",
      "------------------------------------------------\n",
      "lexrank  opinosis  0.0053  -0.0142 0.0249 False \n",
      "lexrank  textrank  0.0618   0.0422 0.0814  True \n",
      "opinosis textrank  0.0565   0.0369 0.076   True \n",
      "------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Set up the data for comparison (creates a specialised object)\n",
    "MultiComp_rouge_2_r = MultiComparison(pd_rouge_2_recall['r'], pd_rouge_2_recall['algorithm'])\n",
    "\n",
    "# Show all pair-wise comparisons:\n",
    "\n",
    "# Print the comparisons\n",
    "\n",
    "print(MultiComp_rouge_2_r.tukeyhsd().summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_rouge_l_f_measure = pd.concat([textrank_rouge_l[['algorithm', 'f']],\n",
    "                                  lexrank_rouge_l[['algorithm', 'f']],\n",
    "                                  opinosis_rouge_l[['algorithm', 'f']]])\n",
    "pd_rouge_l_precision = pd.concat([textrank_rouge_l[['algorithm', 'p']],\n",
    "                                  lexrank_rouge_l[['algorithm', 'p']],\n",
    "                                  opinosis_rouge_l[['algorithm', 'p']]])\n",
    "pd_rouge_l_recall = pd.concat([textrank_rouge_l[['algorithm', 'r']],\n",
    "                                  lexrank_rouge_l[['algorithm', 'r']],\n",
    "                                  opinosis_rouge_l[['algorithm', 'r']]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiple Comparison of Means - Tukey HSD,FWER=0.05\n",
      "=================================================\n",
      " group1   group2  meandiff  lower   upper  reject\n",
      "-------------------------------------------------\n",
      "lexrank  opinosis  0.0196  -0.0009  0.0402 False \n",
      "lexrank  textrank -0.0596  -0.0801  -0.039  True \n",
      "opinosis textrank -0.0792  -0.0998 -0.0587  True \n",
      "-------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Set up the data for comparison (creates a specialised object)\n",
    "MultiComp_rouge_l_f = MultiComparison(pd_rouge_l_f_measure['f'], pd_rouge_l_f_measure['algorithm'])\n",
    "\n",
    "# Show all pair-wise comparisons:\n",
    "\n",
    "# Print the comparisons\n",
    "\n",
    "print(MultiComp_rouge_l_f.tukeyhsd().summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiple Comparison of Means - Tukey HSD,FWER=0.05\n",
      "=================================================\n",
      " group1   group2  meandiff  lower   upper  reject\n",
      "-------------------------------------------------\n",
      "lexrank  opinosis  0.0926   0.0601  0.1251  True \n",
      "lexrank  textrank -0.1116  -0.1441 -0.0791  True \n",
      "opinosis textrank -0.2041  -0.2366 -0.1716  True \n",
      "-------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Set up the data for comparison (creates a specialised object)\n",
    "MultiComp_rouge_l_p = MultiComparison(pd_rouge_l_precision['p'], pd_rouge_l_precision['algorithm'])\n",
    "\n",
    "# Show all pair-wise comparisons:\n",
    "\n",
    "# Print the comparisons\n",
    "\n",
    "print(MultiComp_rouge_l_p.tukeyhsd().summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiple Comparison of Means - Tukey HSD,FWER=0.05\n",
      "================================================\n",
      " group1   group2  meandiff  lower  upper  reject\n",
      "------------------------------------------------\n",
      "lexrank  opinosis  -0.01   -0.0386 0.0187 False \n",
      "lexrank  textrank  0.1641   0.1354 0.1928  True \n",
      "opinosis textrank  0.1741   0.1454 0.2028  True \n",
      "------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Set up the data for comparison (creates a specialised object)\n",
    "MultiComp_rouge_l_r = MultiComparison(pd_rouge_l_recall['r'], pd_rouge_l_recall['algorithm'])\n",
    "\n",
    "# Show all pair-wise comparisons:\n",
    "\n",
    "# Print the comparisons\n",
    "\n",
    "print(MultiComp_rouge_l_r.tukeyhsd().summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f</th>\n",
       "      <th>p</th>\n",
       "      <th>r</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>443.000000</td>\n",
       "      <td>443.000000</td>\n",
       "      <td>443.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.255321</td>\n",
       "      <td>0.275413</td>\n",
       "      <td>0.274900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.186686</td>\n",
       "      <td>0.206015</td>\n",
       "      <td>0.203856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.148148</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.235294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.303030</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                f           p           r\n",
       "count  443.000000  443.000000  443.000000\n",
       "mean     0.255321    0.275413    0.274900\n",
       "std      0.186686    0.206015    0.203856\n",
       "min      0.000000    0.000000    0.000000\n",
       "25%      0.148148    0.153846    0.142857\n",
       "50%      0.222222    0.222222    0.235294\n",
       "75%      0.303030    0.333333    0.333333\n",
       "max      1.000000    1.000000    1.000000"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "human_rouge_1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f</th>\n",
       "      <th>p</th>\n",
       "      <th>r</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>443.000000</td>\n",
       "      <td>443.000000</td>\n",
       "      <td>443.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.087532</td>\n",
       "      <td>0.091925</td>\n",
       "      <td>0.095324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.199262</td>\n",
       "      <td>0.202469</td>\n",
       "      <td>0.211711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                f           p           r\n",
       "count  443.000000  443.000000  443.000000\n",
       "mean     0.087532    0.091925    0.095324\n",
       "std      0.199262    0.202469    0.211711\n",
       "min      0.000000    0.000000    0.000000\n",
       "25%      0.000000    0.000000    0.000000\n",
       "50%      0.000000    0.000000    0.000000\n",
       "75%      0.086957    0.100000    0.090909\n",
       "max      1.000000    1.000000    1.000000"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "human_rouge_2.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f</th>\n",
       "      <th>p</th>\n",
       "      <th>r</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>443.000000</td>\n",
       "      <td>443.000000</td>\n",
       "      <td>443.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.224163</td>\n",
       "      <td>0.263188</td>\n",
       "      <td>0.263414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.187380</td>\n",
       "      <td>0.203533</td>\n",
       "      <td>0.203813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.122363</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.185714</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>0.217391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.267673</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                f           p           r\n",
       "count  443.000000  443.000000  443.000000\n",
       "mean     0.224163    0.263188    0.263414\n",
       "std      0.187380    0.203533    0.203813\n",
       "min      0.000000    0.000000    0.000000\n",
       "25%      0.122363    0.142857    0.142857\n",
       "50%      0.185714    0.208333    0.217391\n",
       "75%      0.267673    0.315789    0.333333\n",
       "max      1.000000    1.000000    1.000000"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "human_rouge_l.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
