{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge import Rouge\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "#goldSummaries = glob.glob(r'..data/raw/OpinosisDataset1.0_0/summaries-gold/*/')\n",
    "gold_summaries = os.walk(r'../data/raw/OpinosisDataset1.0_0/summaries-gold')\n",
    "gold_list = next(gold_summaries)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rouge_evaluation(summaryfiles_dir, summaryDir):\n",
    "    summaryFiles = glob.glob(summaryfiles_dir)\n",
    "    rouge_1_scores_list = []\n",
    "    rouge_2_scores_list = []\n",
    "    rouge_l_scores_list = []\n",
    "    for h in summaryFiles:\n",
    "        foldername_search = re.search(r'[^\\\\/:*?\"<>|\\r\\n]+$', h)\n",
    "        foldername = (foldername_search.group()).split('.')[0]\n",
    "        with open(h, 'r') as f:\n",
    "            hypothesis = f.read()\n",
    "            if foldername in gold_list:\n",
    "                files = glob.glob(summaryDir+foldername+'/*')\n",
    "                foldername = ' '.join(foldername.split('_'))\n",
    "                for r in files:\n",
    "                    with open(r, 'r') as f:\n",
    "                        reference = f.read()\n",
    "                        rouge = Rouge()\n",
    "                        scores = rouge.get_scores(hypothesis, reference)[0]\n",
    "                        rouge_1_scores_list.append(pd.DataFrame(scores['rouge-1'], index=[foldername]))\n",
    "                        rouge_2_scores_list.append(pd.DataFrame(scores['rouge-2'], index=[foldername]))\n",
    "                        rouge_l_scores_list.append(pd.DataFrame(scores['rouge-l'], index=[foldername]))\n",
    "                        rouge_1_df = pd.concat(rouge_1_scores_list)\n",
    "                        rouge_2_df = pd.concat(rouge_2_scores_list)\n",
    "                        rouge_l_df = pd.concat(rouge_l_scores_list)\n",
    "    return rouge_1_df, rouge_2_df, rouge_l_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "def human_rouge():\n",
    "    rouge_1_scores_list = []\n",
    "    rouge_2_scores_list = []\n",
    "    rouge_l_scores_list = []\n",
    "    for folder in gold_list:\n",
    "        allFiles = glob.glob('C:/Users/obiam/MSc_Project/msc_project/data/raw/OpinosisDataset1.0_0/summaries-gold/'+folder + '/*')\n",
    "        for h, r in itertools.combinations(allFiles, 2):\n",
    "            with open(h, 'r') as f:\n",
    "                hypothesis = f.read()\n",
    "            with open(r, 'r') as f:\n",
    "                reference = f.read()\n",
    "                rouge = Rouge()\n",
    "                scores = rouge.get_scores(hypothesis, reference)[0]\n",
    "                rouge_1_scores_list.append(pd.DataFrame(scores['rouge-1'], index=[folder]))\n",
    "                rouge_2_scores_list.append(pd.DataFrame(scores['rouge-2'], index=[folder]))\n",
    "                rouge_l_scores_list.append(pd.DataFrame(scores['rouge-l'], index=[folder]))\n",
    "                rouge_1_df = pd.concat(rouge_1_scores_list)\n",
    "                rouge_2_df = pd.concat(rouge_2_scores_list)\n",
    "                rouge_l_df = pd.concat(rouge_l_scores_list)\n",
    "    return(rouge_1_df, rouge_2_df, rouge_l_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "textrank_rouge_1, textrank_rouge_2, textrank_rouge_l = rouge_evaluation(r'../data/processed/textrank/*', '../data/raw/OpinosisDataset1.0_0/summaries-gold/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexrank_rouge_1, lexrank_rouge_2, lexrank_rouge_l = rouge_evaluation(r'../data/processed/lexrank/*', '../data/raw/OpinosisDataset1.0_0/summaries-gold/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "opinosis_rouge_1, opinosis_rouge_2, opinosis_rouge_l = rouge_evaluation(r'../data/processed/opinosis/*', '../data/raw/OpinosisDataset1.0_0/summaries-gold/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_rouge_1, human_rouge_2, human_rouge_l = human_rouge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import f_oneway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-way ANOVA\n",
      "=============\n",
      "F value: [20.89525625 68.10986342 56.11411163]\n",
      "P value: [3.47669329e-13 1.49822456e-40 7.37098388e-34] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "f, p = f_oneway(textrank_rouge_1, lexrank_rouge_1, opinosis_rouge_1, human_rouge_1)\n",
    "print ('One-way ANOVA')\n",
    "print ('=============')\n",
    " \n",
    "print ('F value:', f)\n",
    "print ('P value:', p, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-way ANOVA\n",
      "=============\n",
      "F value: [12.12490524 14.45845396 12.61935858]\n",
      "P value: [8.14150842e-08 2.99571665e-09 4.04220755e-08] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "f, p = f_oneway(textrank_rouge_2, lexrank_rouge_2, opinosis_rouge_2, human_rouge_2)\n",
    "print ('One-way ANOVA')\n",
    "print ('=============')\n",
    " \n",
    "print ('F value:', f)\n",
    "print ('P value:', p, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-way ANOVA\n",
      "=============\n",
      "F value: [39.00089264 65.8851276  56.55286179]\n",
      "P value: [5.25066593e-24 2.53312482e-39 4.16580614e-34] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "f, p = f_oneway(textrank_rouge_l, lexrank_rouge_l, opinosis_rouge_l, human_rouge_l)\n",
    "print ('One-way ANOVA')\n",
    "print ('=============')\n",
    " \n",
    "print ('F value:', f)\n",
    "print ('P value:', p, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "textrank_rouge_1['algorithm'] = 'textrank'\n",
    "textrank_rouge_2['algorithm'] = 'textrank'\n",
    "textrank_rouge_l['algorithm'] = 'textrank'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexrank_rouge_1['algorithm'] = 'lexrank'\n",
    "lexrank_rouge_2['algorithm'] = 'lexrank'\n",
    "lexrank_rouge_l['algorithm'] = 'lexrank'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "opinosis_rouge_1['algorithm'] = 'opinosis'\n",
    "opinosis_rouge_2['algorithm'] = 'opinosis'\n",
    "opinosis_rouge_l['algorithm'] = 'opinosis'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_rouge_1['algorithm'] = 'human'\n",
    "human_rouge_2['algorithm'] = 'human'\n",
    "human_rouge_l['algorithm'] = 'human'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_rouge_1_f_measure = pd.concat([textrank_rouge_1[['algorithm', 'f']],\n",
    "                                  lexrank_rouge_1[['algorithm', 'f']],\n",
    "                                  opinosis_rouge_1[['algorithm', 'f']],\n",
    "                                 human_rouge_1[['algorithm', 'f']]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_rouge_1_precision = pd.concat([textrank_rouge_1[['algorithm', 'p']],\n",
    "                                  lexrank_rouge_1[['algorithm', 'p']],\n",
    "                                  opinosis_rouge_1[['algorithm', 'p']],\n",
    "                                human_rouge_1[['algorithm', 'p']]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_rouge_1_recall = pd.concat([textrank_rouge_1[['algorithm', 'r']],\n",
    "                                  lexrank_rouge_1[['algorithm', 'r']],\n",
    "                                  opinosis_rouge_1[['algorithm', 'r']],\n",
    "                              human_rouge_1[['algorithm', 'r']]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiple Comparison of Means - Tukey HSD,FWER=0.05\n",
      "=================================================\n",
      " group1   group2  meandiff  lower   upper  reject\n",
      "-------------------------------------------------\n",
      " human   lexrank  -0.0291  -0.0591  0.0009 False \n",
      " human   opinosis  -0.021   -0.051  0.009  False \n",
      " human   textrank -0.0916  -0.1216 -0.0616  True \n",
      "lexrank  opinosis  0.0081  -0.0261  0.0424 False \n",
      "lexrank  textrank -0.0625  -0.0967 -0.0283  True \n",
      "opinosis textrank -0.0706  -0.1049 -0.0364  True \n",
      "-------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.stats.multicomp import (pairwise_tukeyhsd,\n",
    "                                         MultiComparison)\n",
    "\n",
    "# Set up the data for comparison (creates a specialised object)\n",
    "MultiComp_rouge_1_f = MultiComparison(pd_rouge_1_f_measure['f'], pd_rouge_1_f_measure['algorithm'])\n",
    "\n",
    "# Show all pair-wise comparisons:\n",
    "\n",
    "# Print the comparisons\n",
    "\n",
    "print(MultiComp_rouge_1_f.tukeyhsd().summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiple Comparison of Means - Tukey HSD,FWER=0.05\n",
      "=================================================\n",
      " group1   group2  meandiff  lower   upper  reject\n",
      "-------------------------------------------------\n",
      " human   lexrank   -0.02   -0.0569  0.0169 False \n",
      " human   opinosis  0.0524   0.0155  0.0893  True \n",
      " human   textrank -0.1674  -0.2043 -0.1304  True \n",
      "lexrank  opinosis  0.0724   0.0303  0.1145  True \n",
      "lexrank  textrank -0.1474  -0.1895 -0.1052  True \n",
      "opinosis textrank -0.2198  -0.2619 -0.1776  True \n",
      "-------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Set up the data for comparison (creates a specialised object)\n",
    "MultiComp_rouge_1_p = MultiComparison(pd_rouge_1_precision['p'], pd_rouge_1_precision['algorithm'])\n",
    "\n",
    "# Show all pair-wise comparisons:\n",
    "\n",
    "# Print the comparisons\n",
    "print(MultiComp_rouge_1_p.tukeyhsd().summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiple Comparison of Means - Tukey HSD,FWER=0.05\n",
      "=================================================\n",
      " group1   group2  meandiff  lower   upper  reject\n",
      "-------------------------------------------------\n",
      " human   lexrank  -0.0322  -0.0672  0.0027 False \n",
      " human   opinosis -0.0632  -0.0981 -0.0282  True \n",
      " human   textrank  0.1239   0.089   0.1589  True \n",
      "lexrank  opinosis -0.0309  -0.0708  0.0089 False \n",
      "lexrank  textrank  0.1562   0.1163  0.196   True \n",
      "opinosis textrank  0.1871   0.1472  0.227   True \n",
      "-------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Set up the data for comparison (creates a specialised object)\n",
    "MultiComp_rouge_1_r = MultiComparison(pd_rouge_1_recall['r'], pd_rouge_1_recall['algorithm'])\n",
    "\n",
    "# Show all pair-wise comparisons:\n",
    "\n",
    "# Print the comparisons\n",
    "print(MultiComp_rouge_1_r.tukeyhsd().summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_rouge_2_f_measure = pd.concat([textrank_rouge_2[['algorithm', 'f']],\n",
    "                                  lexrank_rouge_2[['algorithm', 'f']],\n",
    "                                  opinosis_rouge_2[['algorithm', 'f']],\n",
    "                                  human_rouge_2[['algorithm', 'f']]])\n",
    "pd_rouge_2_precision = pd.concat([textrank_rouge_2[['algorithm', 'p']],\n",
    "                                  lexrank_rouge_2[['algorithm', 'p']],\n",
    "                                  opinosis_rouge_2[['algorithm', 'p']],\n",
    "                                  human_rouge_2[['algorithm', 'p']]])\n",
    "pd_rouge_2_recall = pd.concat([textrank_rouge_2[['algorithm', 'r']],\n",
    "                                  lexrank_rouge_2[['algorithm', 'r']],\n",
    "                                  opinosis_rouge_2[['algorithm', 'r']],\n",
    "                               human_rouge_2[['algorithm', 'r']]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiple Comparison of Means - Tukey HSD,FWER=0.05\n",
      "=================================================\n",
      " group1   group2  meandiff  lower   upper  reject\n",
      "-------------------------------------------------\n",
      " human   lexrank  -0.0501  -0.0779 -0.0223  True \n",
      " human   opinosis -0.0371  -0.0649 -0.0092  True \n",
      " human   textrank -0.0553  -0.0831 -0.0275  True \n",
      "lexrank  opinosis  0.013   -0.0187  0.0447 False \n",
      "lexrank  textrank -0.0052  -0.0369  0.0265 False \n",
      "opinosis textrank -0.0182   -0.05   0.0135 False \n",
      "-------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Set up the data for comparison (creates a specialised object)\n",
    "MultiComp_rouge_2_f = MultiComparison(pd_rouge_2_f_measure['f'], pd_rouge_2_f_measure['algorithm'])\n",
    "\n",
    "# Show all pair-wise comparisons:\n",
    "\n",
    "# Print the comparisons\n",
    "\n",
    "print(MultiComp_rouge_2_f.tukeyhsd().summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiple Comparison of Means - Tukey HSD,FWER=0.05\n",
      "=================================================\n",
      " group1   group2  meandiff  lower   upper  reject\n",
      "-------------------------------------------------\n",
      " human   lexrank  -0.0479  -0.0789  -0.017  True \n",
      " human   opinosis -0.0097  -0.0407  0.0212 False \n",
      " human   textrank -0.0716  -0.1025 -0.0407  True \n",
      "lexrank  opinosis  0.0382   0.0029  0.0735  True \n",
      "lexrank  textrank -0.0237   -0.059  0.0116 False \n",
      "opinosis textrank -0.0619  -0.0972 -0.0266  True \n",
      "-------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Set up the data for comparison (creates a specialised object)\n",
    "MultiComp_rouge_2_p = MultiComparison(pd_rouge_2_precision['p'], pd_rouge_2_precision['algorithm'])\n",
    "\n",
    "# Show all pair-wise comparisons:\n",
    "\n",
    "# Print the comparisons\n",
    "\n",
    "print(MultiComp_rouge_2_p.tukeyhsd().summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiple Comparison of Means - Tukey HSD,FWER=0.05\n",
      "=================================================\n",
      " group1   group2  meandiff  lower   upper  reject\n",
      "-------------------------------------------------\n",
      " human   lexrank  -0.0544  -0.0853 -0.0235  True \n",
      " human   opinosis -0.0526  -0.0835 -0.0217  True \n",
      " human   textrank  0.0038  -0.0271  0.0347 False \n",
      "lexrank  opinosis  0.0018  -0.0335  0.0371 False \n",
      "lexrank  textrank  0.0583   0.023   0.0935  True \n",
      "opinosis textrank  0.0565   0.0212  0.0917  True \n",
      "-------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Set up the data for comparison (creates a specialised object)\n",
    "MultiComp_rouge_2_r = MultiComparison(pd_rouge_2_recall['r'], pd_rouge_2_recall['algorithm'])\n",
    "\n",
    "# Show all pair-wise comparisons:\n",
    "\n",
    "# Print the comparisons\n",
    "\n",
    "print(MultiComp_rouge_2_r.tukeyhsd().summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_rouge_l_f_measure = pd.concat([textrank_rouge_l[['algorithm', 'f']],\n",
    "                                  lexrank_rouge_l[['algorithm', 'f']],\n",
    "                                  opinosis_rouge_l[['algorithm', 'f']],\n",
    "                                  human_rouge_l[['algorithm', 'f']]])\n",
    "pd_rouge_l_precision = pd.concat([textrank_rouge_l[['algorithm', 'p']],\n",
    "                                  lexrank_rouge_l[['algorithm', 'p']],\n",
    "                                  opinosis_rouge_l[['algorithm', 'p']],\n",
    "                                  human_rouge_l[['algorithm', 'p']]])\n",
    "pd_rouge_l_recall = pd.concat([textrank_rouge_l[['algorithm', 'r']],\n",
    "                                  lexrank_rouge_l[['algorithm', 'r']],\n",
    "                                  opinosis_rouge_l[['algorithm', 'r']],\n",
    "                               human_rouge_l[['algorithm', 'r']]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiple Comparison of Means - Tukey HSD,FWER=0.05\n",
      "=================================================\n",
      " group1   group2  meandiff  lower   upper  reject\n",
      "-------------------------------------------------\n",
      " human   lexrank  -0.0556  -0.0841 -0.0271  True \n",
      " human   opinosis -0.0397  -0.0682 -0.0111  True \n",
      " human   textrank -0.1189  -0.1474 -0.0903  True \n",
      "lexrank  opinosis  0.0159  -0.0166  0.0485 False \n",
      "lexrank  textrank -0.0633  -0.0958 -0.0307  True \n",
      "opinosis textrank -0.0792  -0.1118 -0.0467  True \n",
      "-------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Set up the data for comparison (creates a specialised object)\n",
    "MultiComp_rouge_l_f = MultiComparison(pd_rouge_l_f_measure['f'], pd_rouge_l_f_measure['algorithm'])\n",
    "\n",
    "# Show all pair-wise comparisons:\n",
    "\n",
    "# Print the comparisons\n",
    "\n",
    "print(MultiComp_rouge_l_f.tukeyhsd().summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiple Comparison of Means - Tukey HSD,FWER=0.05\n",
      "=================================================\n",
      " group1   group2  meandiff  lower   upper  reject\n",
      "-------------------------------------------------\n",
      " human   lexrank  -0.0488  -0.0845  -0.013  True \n",
      " human   opinosis  0.0399   0.0042  0.0756  True \n",
      " human   textrank -0.1642  -0.1999 -0.1285  True \n",
      "lexrank  opinosis  0.0887   0.0479  0.1294  True \n",
      "lexrank  textrank -0.1155  -0.1562 -0.0747  True \n",
      "opinosis textrank -0.2041  -0.2449 -0.1634  True \n",
      "-------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Set up the data for comparison (creates a specialised object)\n",
    "MultiComp_rouge_l_p = MultiComparison(pd_rouge_l_precision['p'], pd_rouge_l_precision['algorithm'])\n",
    "\n",
    "# Show all pair-wise comparisons:\n",
    "\n",
    "# Print the comparisons\n",
    "\n",
    "print(MultiComp_rouge_l_p.tukeyhsd().summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiple Comparison of Means - Tukey HSD,FWER=0.05\n",
      "=================================================\n",
      " group1   group2  meandiff  lower   upper  reject\n",
      "-------------------------------------------------\n",
      " human   lexrank  -0.0595  -0.0934 -0.0257  True \n",
      " human   opinosis -0.0704  -0.1043 -0.0366  True \n",
      " human   textrank  0.1037   0.0698  0.1375  True \n",
      "lexrank  opinosis -0.0109  -0.0495  0.0277 False \n",
      "lexrank  textrank  0.1632   0.1246  0.2018  True \n",
      "opinosis textrank  0.1741   0.1355  0.2127  True \n",
      "-------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Set up the data for comparison (creates a specialised object)\n",
    "MultiComp_rouge_l_r = MultiComparison(pd_rouge_l_recall['r'], pd_rouge_l_recall['algorithm'])\n",
    "\n",
    "# Show all pair-wise comparisons:\n",
    "\n",
    "# Print the comparisons\n",
    "\n",
    "print(MultiComp_rouge_l_r.tukeyhsd().summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f</th>\n",
       "      <th>p</th>\n",
       "      <th>r</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>443.000</td>\n",
       "      <td>443.000</td>\n",
       "      <td>443.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.255</td>\n",
       "      <td>0.275</td>\n",
       "      <td>0.275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.187</td>\n",
       "      <td>0.206</td>\n",
       "      <td>0.204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.148</td>\n",
       "      <td>0.154</td>\n",
       "      <td>0.143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.222</td>\n",
       "      <td>0.222</td>\n",
       "      <td>0.235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.303</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             f        p        r\n",
       "count  443.000  443.000  443.000\n",
       "mean     0.255    0.275    0.275\n",
       "std      0.187    0.206    0.204\n",
       "min      0.000    0.000    0.000\n",
       "25%      0.148    0.154    0.143\n",
       "50%      0.222    0.222    0.235\n",
       "75%      0.303    0.333    0.333\n",
       "max      1.000    1.000    1.000"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "human_rouge_1.describe().round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f</th>\n",
       "      <th>p</th>\n",
       "      <th>r</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>443.000</td>\n",
       "      <td>443.000</td>\n",
       "      <td>443.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.088</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.199</td>\n",
       "      <td>0.202</td>\n",
       "      <td>0.212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.087</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             f        p        r\n",
       "count  443.000  443.000  443.000\n",
       "mean     0.088    0.092    0.095\n",
       "std      0.199    0.202    0.212\n",
       "min      0.000    0.000    0.000\n",
       "25%      0.000    0.000    0.000\n",
       "50%      0.000    0.000    0.000\n",
       "75%      0.087    0.100    0.091\n",
       "max      1.000    1.000    1.000"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "human_rouge_2.describe().round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f</th>\n",
       "      <th>p</th>\n",
       "      <th>r</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>443.000</td>\n",
       "      <td>443.000</td>\n",
       "      <td>443.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.224</td>\n",
       "      <td>0.263</td>\n",
       "      <td>0.263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.187</td>\n",
       "      <td>0.204</td>\n",
       "      <td>0.204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.122</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.186</td>\n",
       "      <td>0.208</td>\n",
       "      <td>0.217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.268</td>\n",
       "      <td>0.316</td>\n",
       "      <td>0.333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             f        p        r\n",
       "count  443.000  443.000  443.000\n",
       "mean     0.224    0.263    0.263\n",
       "std      0.187    0.204    0.204\n",
       "min      0.000    0.000    0.000\n",
       "25%      0.122    0.143    0.143\n",
       "50%      0.186    0.208    0.217\n",
       "75%      0.268    0.316    0.333\n",
       "max      1.000    1.000    1.000"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "human_rouge_l.describe().round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
